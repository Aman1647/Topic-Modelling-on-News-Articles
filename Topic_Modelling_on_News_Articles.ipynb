{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aman1647/Topic-Modelling-on-News-Articles/blob/main/Topic_Modelling_on_News_Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "aybLGuIBys65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CRVl-OBJy2W5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Know Your Data"
      ],
      "metadata": {
        "id": "8RUD6uJjy3NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Hh9UryQTywNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing Libraries"
      ],
      "metadata": {
        "id": "RwLFWdUIzODj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhOp2tP7ypel"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display  # interactive display\n",
        "from tqdm import tqdm   #progress bar of execution\n",
        "from collections import Counter  # when u want to iterate over something and keep a count of that\n",
        "import ast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sns\n",
        "import os                                                                        # for listing files in given directory\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer     # to create a dict --> how many times which word has occured in the document\n",
        "from textblob import TextBlob\n",
        "import scipy.stats as stats\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD              # Singular value decomposition\n",
        "from sklearn.decomposition import LatentDirichletAllocation  \n",
        "from sklearn.manifold import TSNE    # similar to PCA --> used for dimensionality reduction\n",
        "\n",
        "#output_notebook()\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing packages"
      ],
      "metadata": {
        "id": "YnMzLa3GnIUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions                                                        # for removing contractions\n",
        "!pip install pyLDAvis  "
      ],
      "metadata": {
        "id": "kuULzPFxnHeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Almabetter/Capstone Projects/Unsupervised Learning/Topic Modelling/bbc/'\n",
        "folders = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]"
      ],
      "metadata": {
        "id": "AIzHKVdxzStp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = []                                                                        \n",
        "topics = []\n",
        "\n",
        "for i in folders:\n",
        "  lst_file_names = os.listdir(path+i)                                            # list of file names present in directory bbc\n",
        "  for txt_files in lst_file_names:\n",
        "    txt_path = path + i+ '/'+ txt_files                                          # exact path of all text files\n",
        "    with open(txt_path, 'rb') as f:                                              # open a binary file\n",
        "      text = f.read()                                                            # read all lines\n",
        "      news.append(text)                                                          # append text files \n",
        "      topics.append(i)                                                           # append topics \n"
      ],
      "metadata": {
        "id": "v3V9ZSDjz-NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe of news & type\n",
        "df = pd.DataFrame()\n",
        "df['News_text'] = news\n",
        "df['type'] = topics"
      ],
      "metadata": {
        "id": "iRkqPvKo64lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "WWbPfcw3BNS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "mEUrUUKLoNIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "_S5nV7bMoPUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "1BNul23Aq7ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "VtWGeYk2oSqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "dBqAIu6Tyzxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling"
      ],
      "metadata": {
        "id": "9P5_eEV93C99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding length of each news\n",
        "df['length'] = df['News_text'].apply(len)"
      ],
      "metadata": {
        "id": "H2xN7U-83FEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding word countof each news\n",
        "df['word_count'] = df['News_text'].apply(lambda x:len(str(x).split(\" \")))"
      ],
      "metadata": {
        "id": "6rFFlfFx3rWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "cIvoGzYU36mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA"
      ],
      "metadata": {
        "id": "DRXxNBd6y2rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df = df.copy()"
      ],
      "metadata": {
        "id": "4ZoWETQjy-L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_count = bbc_df['type'].value_counts()\n",
        "topic_count"
      ],
      "metadata": {
        "id": "d--Hq6g9zsWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 8))\n",
        "plt.pie(topic_count , labels = topic_count.index, autopct = '%0.2f%%' )\n",
        "plt.title(\"Topic Distribution\", size=15)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "PXEjpLmK0rCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# length of news in each type\n",
        "plt.figure(figsize = (10, 8))\n",
        "sns.barplot(x= bbc_df['type'], y= bbc_df['length'])\n",
        "plt.title('Length of News in each type', size=15)\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel(\"Length of news\")\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "d4BSaFZK4nqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# length of news in each type\n",
        "plt.figure(figsize = (10, 8))\n",
        "sns.barplot(x= bbc_df['type'], y= bbc_df['word_count'])\n",
        "plt.title('Word count of News in each type', size=15)\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel(\"Word count of news\")\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "ESPy8PY65fAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing & Duplicate values"
      ],
      "metadata": {
        "id": "foVeYVeo295Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping duplicate values\n",
        "bbc_df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "KEb5B53y2n9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Pre-processing"
      ],
      "metadata": {
        "id": "MNlNmXLC_n31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df.head()"
      ],
      "metadata": {
        "id": "odYzRShlAJlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "WU3tPxka__J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lower casing"
      ],
      "metadata": {
        "id": "KXMCQQw6TU2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "bbc_df['News_text']= bbc_df['News_text'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "gtQ6K0rACClD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing HTML tags\n",
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove html tags from a string\"\"\"\n",
        "    import re                                                                    # regular expression module\n",
        "    clean = re.compile('<.*?>')                                                  # removes anything in < >\n",
        "    return re.sub(clean, '', text)"
      ],
      "metadata": {
        "id": "AHKrbkTbDFcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = bbc_df['News_text'].apply(remove_html_tags)"
      ],
      "metadata": {
        "id": "d5F8F6g7bWyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing url's\n",
        "def remove_urls(text):\n",
        "  \"\"\" Remove url tags from a string \"\"\"\n",
        "  url_pattern = r\"https?://+|www\\.\"                       # \\S+ --> matches anything non-white space character with repetations ; ? --> matches 0 or 1 occurences of pattern to  its left \n",
        "  without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
        "  return without_urls"
      ],
      "metadata": {
        "id": "A0IWc25xVevx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = bbc_df['News_text'].apply(remove_urls)"
      ],
      "metadata": {
        "id": "rDt1SadPcBDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "ZwEwJARzoLou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing non-word \n",
        "def rem_non_word(text):\n",
        "  \"\"\" Remove non-word characters from the string \"\"\"\n",
        "  non_words = r\"\\s+[a-zA-Z]'\\s+\"                                               # \\s+ --> matches space character with repetation(+) ; [a-zA-Z] --> match text string within range\n",
        "  without_nw = re.sub(pattern = non_words , repl = '', string =text)\n",
        "  return without_nw"
      ],
      "metadata": {
        "id": "RrIcViUTXt8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = bbc_df['News_text'].apply(rem_non_word)"
      ],
      "metadata": {
        "id": "_UgsdK2-YZQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = [re.sub(r\"\\\\n+\", \" \", i) for i in bbc_df['News_text']]"
      ],
      "metadata": {
        "id": "2BBb6OOOo0H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "YJdDjMqjP8KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = [re.sub(r\"b\\'+\", \"\", i) for i in bbc_df['News_text']]"
      ],
      "metadata": {
        "id": "-OEdx4-tl_dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "IVLLzSHsQO0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = [re.sub(r\"\\\\\\'s+\", \"\", i) for i in bbc_df['News_text']]"
      ],
      "metadata": {
        "id": "ZRPcAuTwmGUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "XCsPElp5nerl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Removing punctuations"
      ],
      "metadata": {
        "id": "91Dcrg1PTW5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#library that contains punctuation\n",
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "id": "j1H-jdS1TUO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "  \"\"\" Removes punctuations mentioned in the library \"\"\"\n",
        "  punctuation_free = \"\".join([i for i in text if i not in string.punctuation])    # joins everything except punctuations\n",
        "  return punctuation_free"
      ],
      "metadata": {
        "id": "FaSSxgwrT1nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = bbc_df['News_text'].apply(lambda x:remove_punctuation(x))"
      ],
      "metadata": {
        "id": "pSdTV4PQUe-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "vJV8cCYZUpQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Remove Numbers"
      ],
      "metadata": {
        "id": "lCnKYVKcVdKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(text):\n",
        "  num_to_remove = r'\\d+'                                                               # \\d+ --> matches digits 0-9 with any repetation\n",
        "  without_num = re.sub(pattern = num_to_remove , repl =\"\", string=text)\n",
        "  return without_num"
      ],
      "metadata": {
        "id": "c9hTGepGVafv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = bbc_df['News_text'].apply(remove_numbers)"
      ],
      "metadata": {
        "id": "ggS2Imu1ZHme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "Roij8tqeZObf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tokenization"
      ],
      "metadata": {
        "id": "ZSCnn3Ukgyxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Word tokenization function\n",
        "def tokenization(text):\n",
        "    tokens = re.split('\\W+',text)                           # creates words as tokens\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "_5d_80hbdGmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['tokens'] = bbc_df['News_text'].apply(tokenization)"
      ],
      "metadata": {
        "id": "0adIhJtqiOig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df.head()"
      ],
      "metadata": {
        "id": "CEwRF7L7iWcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Removing Stop-words"
      ],
      "metadata": {
        "id": "Af01zHg2jOtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "MVuNVOWgiY0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the stopwords from nltk library\n",
        "sw = stopwords.words('english')\n",
        "# displaying the stopwords\n",
        "np.array(sw)"
      ],
      "metadata": {
        "id": "LhJ6qvdFkGPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    text = [word for word in text.split() if word not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "uzpqhVyLkbNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_wo_stopwords'] = bbc_df['News_text'].apply(stopwords)"
      ],
      "metadata": {
        "id": "XHC16P1hlPPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df.head()"
      ],
      "metadata": {
        "id": "jIKiKC2glque"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lemmatization"
      ],
      "metadata": {
        "id": "xA613Q70m29Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the object for Lemmatization\n",
        "lmt = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "jkWfqOz8nGu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the function for lemmatization\n",
        "def lemmatizer(text):\n",
        "  \"\"\" This function lematizes each word in the text \"\"\"\n",
        "  lemat_text = [lmt.lemmatize(word) for word in text.split()]\n",
        "  return lemat_text"
      ],
      "metadata": {
        "id": "Vzb5R0bSqI6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['lematized_news'] = bbc_df['News_text'].apply(lambda x:lemmatizer(x))"
      ],
      "metadata": {
        "id": "i_BWeTYNqj3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df.head()"
      ],
      "metadata": {
        "id": "2yUSgh9yq37_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Clean News"
      ],
      "metadata": {
        "id": "_d1TuxORvt06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['clean_news'] = [' '.join(text) for text in bbc_df['lematized_news']] "
      ],
      "metadata": {
        "id": "rm1QuHhivxOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['clean_news'][2]"
      ],
      "metadata": {
        "id": "nw1kSZJ7w1je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* POS Tagging"
      ],
      "metadata": {
        "id": "FV-YZTb2OKjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using spacy library\n",
        "\n",
        "def pos_tagging(text):\n",
        "  allowed_postags = ['NOUN', 'ADJ']\n",
        "  tag_txt = []\n",
        "  doc = nlp(text)\n",
        "  tag_txt.append([token.text for token in doc if token.pos_ in allowed_postags])   \n",
        "  return ' '.join(tag_txt[0])\n"
      ],
      "metadata": {
        "id": "jDsDSdWOOIto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\" ,disable=['parser', 'ner'])\n",
        "bbc_df['news'] = bbc_df['clean_news'].apply(lambda x:pos_tagging(x))"
      ],
      "metadata": {
        "id": "mGq9eO_qOM7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df.head()"
      ],
      "metadata": {
        "id": "8HE2Pb7dP9A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['news'][0]"
      ],
      "metadata": {
        "id": "D_bkGfRJQyHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top Words"
      ],
      "metadata": {
        "id": "OtSgwEIlTg7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
        "    '''\n",
        "    returns a tuple of the top n words in a sample and their \n",
        "    accompanying counts, given a CountVectorizer object and text sample\n",
        "    '''\n",
        "    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)        # .values --> creates a numpy array\n",
        "    vectorized_total = np.sum(vectorized_headlines, axis=0)\n",
        "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)           # index / position of each word in all documents\n",
        "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)                # values of words at that position\n",
        "    \n",
        "    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))         # n top words binary matrix for all the headlines\n",
        "    for i in range(n_top_words):\n",
        "        word_vectors[i,word_indices[0,i]] = 1\n",
        "\n",
        "    words = [word[0].encode('ascii').decode('utf-8') for                           # n top words \n",
        "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
        "\n",
        "    return (words, word_values[0,:n_top_words].tolist()[0])"
      ],
      "metadata": {
        "id": "BBiSAFfUTzDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english')"
      ],
      "metadata": {
        "id": "h157hIx5qpbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "words, word_values = get_top_n_words(n_top_words=15,\n",
        "                                     count_vectorizer=count_vectorizer, \n",
        "                                     text_data=bbc_df['news'])"
      ],
      "metadata": {
        "id": "PdMWI57urMAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "pD5aXMkRrNyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "words, word_values = get_top_n_words(n_top_words=15,\n",
        "                                     count_vectorizer=count_vectorizer, \n",
        "                                     text_data=bbc_df['news'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.bar(range(len(words)), word_values)\n",
        "ax.set_xticks(range(len(words)))\n",
        "ax.set_xticklabels(words, rotation='vertical')\n",
        "ax.set_title('Top words in headlines dataset (excluding stop words)', size = 15)\n",
        "ax.set_xlabel('Word', size =12 )\n",
        "ax.set_ylabel('Number of occurences', size=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qm7buHzPqrvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word-Cloud"
      ],
      "metadata": {
        "id": "zIFMgbGCz4zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "cl_words = ' '.join(bbc_df['news'])\n",
        "wordCloud = WordCloud(width=800, height=500, background_color=\"black\", max_font_size=100).generate(cl_words)\n",
        "plt.imshow(wordCloud, interpolation=\"bilinear\", cmap = 'BuPu')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a2aBVz1Vz8Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "allWords = ' '.join([topic for topic in bbc_df[bbc_df['type']=='business']['news']])\n",
        "wordCloud = WordCloud(width=500, height=300, background_color=\"black\", random_state=21, max_font_size=100).generate(allWords)\n",
        "plt.imshow(wordCloud, interpolation=\"bilinear\", cmap = 'Greys')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EpFKALDG2I6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text-Vectorization"
      ],
      "metadata": {
        "id": "If9D9VOc4Lpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Count Vectorization"
      ],
      "metadata": {
        "id": "10EWdT-0eMYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english', max_features=4000)                         ## Common for all models\n",
        "count_data = count_vectorizer.fit_transform(bbc_df['news'])\n",
        "feature_names = count_vectorizer.get_feature_names()\n",
        "number_topics = 5\n",
        "top_words = 15"
      ],
      "metadata": {
        "id": "4A8wsBB24Ocq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Headline after vectorization : \\n{}'.format(feature_names))"
      ],
      "metadata": {
        "id": "k73Uw3tPOOOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Model Implementation"
      ],
      "metadata": {
        "id": "Ra-QxqVgQFY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 1. Latent Dirichlet Allocation (LDA)  \n",
        "\n"
      ],
      "metadata": {
        "id": "e7hFsmE8QJcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter tuning \n",
        "from sklearn.model_selection import GridSearchCV \n",
        "grid_params = {'n_components':range(5, 10)}"
      ],
      "metadata": {
        "id": "sTo-13KeQbR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model\n",
        "lda = LatentDirichletAllocation()\n",
        "lda_model = GridSearchCV(lda, param_grid = grid_params)\n",
        "lda_model.fit(count_data)"
      ],
      "metadata": {
        "id": "FFBysyQGRDyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best LDA model\n",
        "best_lda_model = lda_model.best_estimator_\n",
        "\n",
        "print(\"Best LDA model's params\" , lda_model.best_params_)\n",
        "print(\"Best log likelihood Score for the LDA model\",lda_model.best_score_)\n",
        "print(\"LDA model Perplexity on train data\", best_lda_model.perplexity(count_data))"
      ],
      "metadata": {
        "id": "a3OPsXCURgrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.sklearn\n",
        "\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "FVJXSbWCSrTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_panel = pyLDAvis.sklearn.prepare(best_lda_model, count_data ,count_vectorizer,mds='tsne')\n",
        "lda_panel"
      ],
      "metadata": {
        "id": "Ezhu0DTKSwEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LDA correctly describes the topics that we most, predicted*"
      ],
      "metadata": {
        "id": "pYDZeRCiUHkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Latent Semantic Analysis (LSA)"
      ],
      "metadata": {
        "id": "UbJv1b5sUUam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* using Count vectorization"
      ],
      "metadata": {
        "id": "jtB6vVXqZNXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "metadata": {
        "id": "CgSpLSXdZUWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# document clustering for LSA\n",
        "tsvd = TruncatedSVD(n_components = 5)\n",
        "tsvd.fit(count_data)\n",
        "tsvd_mat = tsvd.transform(count_data)"
      ],
      "metadata": {
        "id": "-Fb_SMfhUPwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_list =[]\n",
        "\n",
        "for clus in tqdm(range(2, 21)):\n",
        "  km = KMeans(n_clusters=clus, n_init=50, max_iter=1000)                         # Instantiate KMeans clustering\n",
        "  km.fit(tsvd_mat)                                                               # Run KMeans clustering\n",
        "  s = silhouette_score(tsvd_mat, km.labels_)\n",
        "  s_list.append(s)"
      ],
      "metadata": {
        "id": "dSpQJtaYZ6wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(2,21), s_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2_fW0l1jbNTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TSNE plot\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_mat = tsne.fit_transform(tsvd_mat)"
      ],
      "metadata": {
        "id": "-cLSFib6bxs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.scatterplot(tsne_mat[:,0],tsne_mat[:,1],hue=bbc_df['type'])"
      ],
      "metadata": {
        "id": "AwLpoRHVb0RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_top_words_dict(model, features , n_top_words):                        # Defining function for top words\n",
        "    \"\"\" This function gives top words.\"\"\"\n",
        "    top_words_dict = {}\n",
        "    for topic_id, topic in enumerate(model.components_):\n",
        "        top_words_dict[topic_id] = [features[i] for i in topic.argsort()[:-n_top_words - 1:-1]]        \n",
        "    return top_words_dict"
      ],
      "metadata": {
        "id": "g4-67kZ5j2uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 20 words bagged by SVD model using CountVectorizer\")\n",
        "\n",
        "svd_top_words = create_top_words_dict(tsvd, feature_names , top_words)\n",
        "\n",
        "print(svd_top_words)"
      ],
      "metadata": {
        "id": "LSXD7WmEkPsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)    "
      ],
      "metadata": {
        "id": "d6GjRi4gk7h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd_top_words = create_top_words_dict(tsvd, feature_names , top_words)\n",
        "svd_df = pd.DataFrame([svd_top_words]).T\n",
        "svd_df.rename(columns = {0: 'Top 20 words'}, inplace = True)\n",
        "svd_df.rename(index = {0: 'Topic 1', 1: 'Topic 2' , 2: 'Topic 3' , 3: 'Topic 4' , 4 : 'Topic 5'}, inplace = True)\n",
        "svd_df"
      ],
      "metadata": {
        "id": "S-tlrT5Pj5Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using TF-IDF "
      ],
      "metadata": {
        "id": "j9UjH3lumQic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.8, max_features=8000,min_df=0.05)\n",
        "tfidf_matrix = vectorizer.fit_transform(bbc_df['news'])"
      ],
      "metadata": {
        "id": "F7Qiy8ecmcqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "id": "g9ug7B6SnzIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# svd model\n",
        "\n",
        "svd_model = TruncatedSVD(n_components = 5, algorithm ='randomized', n_iter =100, random_state = 0)\n",
        "svd_model.fit(tfidf_matrix)"
      ],
      "metadata": {
        "id": "yof-m2sooGiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i, comp in enumerate(svd_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:15]\n",
        "    print(\"Topic \\n\" +str(i)+\" \" )\n",
        "    for t in sorted_terms:\n",
        "        print(t[0],end=\" \")\n"
      ],
      "metadata": {
        "id": "wYvbBqmvo-Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. LDA using Gensim library"
      ],
      "metadata": {
        "id": "Tr6R7LPmsRAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora"
      ],
      "metadata": {
        "id": "1YsLMEFHC4Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating word vector for gensim \n",
        "dtm_g = bbc_df['news'].str.split().tolist()"
      ],
      "metadata": {
        "id": "VDhcu2FJDzd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtm_g[:1]"
      ],
      "metadata": {
        "id": "nk5F8TpeG1Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word = corpora.Dictionary(dtm_g)                                                # mapping words to tokens\n",
        "# counting the number of occurrences of each distinct word,--> converting to its integer word id and return the result as a sparse vector.\n",
        "corpus = [id2word.doc2bow(text) for text in dtm_g]                                "
      ],
      "metadata": {
        "id": "mg_w5OqCEht-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus[:120])"
      ],
      "metadata": {
        "id": "xzkWpq1AFxuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "metadata": {
        "id": "IzDfIVsPGOvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_g = gensim.models.ldamodel.LdaModel                                           # creating object for lda using gensim library\n",
        "lda_model = lda_g(corpus = corpus, num_topics = 5, id2word=id2word, random_state= 101, chunksize = 500 , passes=10 , eval_every =None )"
      ],
      "metadata": {
        "id": "Re4cZj-1pSUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.print_topics()"
      ],
      "metadata": {
        "id": "sOBqP8BKHbT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The above output means: each of the unique words are given weights based on the topics.This implies which of the words dominate the topics.*"
      ],
      "metadata": {
        "id": "gKGyKUm-Tclj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda_model.print_topics(num_topics=6, num_words=5))"
      ],
      "metadata": {
        "id": "ESoIc3GvUAuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim_models"
      ],
      "metadata": {
        "id": "WdF7HhpKTnct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis=pyLDAvis.gensim_models.prepare(lda_model,corpus,id2word)\n",
        "vis"
      ],
      "metadata": {
        "id": "Deqiv7PDTTDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=dtm_g, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "Vc3LcKr_i-MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion**"
      ],
      "metadata": {
        "id": "CXTyEhE2aH8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Overall 3 different algorithms were used for evaluation of topic modelling.\n",
        "* LDA provided considerable result , with distinct topics as expected.\n",
        "* LSA with CountVectorizer gave somoewhat considerable results, but with TF-IDF the results were not as expected.\n",
        "* LDA using gensim library provided the best result with a considerable coherence score of 0.52 \n",
        "* Topics represented in LDA were adjacent, with hidden topics and relationship between words and documents were found with multiple probability distribution\n"
      ],
      "metadata": {
        "id": "RQgh1OFnxCgN"
      }
    }
  ]
}